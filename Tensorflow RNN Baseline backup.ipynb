{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "#pd.options.display.max_columns = None\n",
    "#pd.options.display.mpl_style = 'default'\n",
    "\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_title_char = np.load('preprocessed_data/train_title_char.npy')\n",
    "train_title_word = np.load('preprocessed_data/train_title_word.npy')\n",
    "test_title_char = np.load('preprocessed_data/test_title_char.npy')\n",
    "test_title_word = np.load('preprocessed_data/test_title_word.npy')\n",
    "\n",
    "train_labels = np.load('preprocessed_data/labels.npy').item()\n",
    "label_dictionary = np.load('preprocessed_data/label_dictionary.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_embedding = np.load('preprocessed_data/char_embedding.npy')\n",
    "word_embedding = np.load('preprocessed_data/word_embedding.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_len = np.load('preprocessed_data/train_length.npy')\n",
    "test_len = np.load('preprocessed_data/test_length.npy')\n",
    "\n",
    "train_title_char_len = train_len[0]\n",
    "train_title_word_len = train_len[1]\n",
    "test_title_char_len = test_len[0]\n",
    "test_title_word_len = test_len[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18378, 256)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2999967, 35)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_title_word.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title = 'char'\n",
    "train_size = train_title_char.shape[0]\n",
    "\n",
    "if title  == 'char':\n",
    "    train_title = train_title_char[:round(train_size*9/10)]\n",
    "    train_title_length = train_title_char_len[:round(train_size*9/10)]\n",
    "    train_label = train_labels[:round(train_size*9/10)]\n",
    "    val_title = train_title_char[round(train_size*9/10):]\n",
    "    val_title_length = train_title_char_len[round(train_size*9/10):]\n",
    "    val_label = train_labels[round(train_size*9/10):]\n",
    "    embedding = char_embedding\n",
    "else:\n",
    "    train_title = train_title_word[:round(train_size*9/10)]\n",
    "    train_title_length = train_title_word_len[:round(train_size*9/10)]\n",
    "    train_label = train_labels[:round(train_size*9/10)]\n",
    "    val_title = train_title_word[round(train_size*9/10):]\n",
    "    val_title_length = train_title_word_len[round(train_size*9/10):]\n",
    "    val_label = train_labels[round(train_size*9/10):]\n",
    "    embedding = word_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title Charactor RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evalation(predict_label_and_marked_label_list):\n",
    "\n",
    "    right_label_num = 0  \n",
    "    right_label_at_pos_num = [0, 0, 0, 0, 0] \n",
    "    sample_num = 0   \n",
    "    all_marked_label_num = 0    \n",
    "    for predict_labels, marked_labels in predict_label_and_marked_label_list:\n",
    "        sample_num += 1\n",
    "        marked_label_set = set(marked_labels)\n",
    "        all_marked_label_num += len(marked_label_set)\n",
    "        for pos, label in zip(range(0, min(len(predict_labels), 5)), predict_labels):\n",
    "            if label in marked_label_set:     \n",
    "                right_label_num += 1\n",
    "                right_label_at_pos_num[pos] += 1\n",
    "\n",
    "    precision = 0.0\n",
    "    for pos, right_num in zip(range(0, 5), right_label_at_pos_num):\n",
    "        precision += ((right_num / float(sample_num))) / np.log(2.0 + pos)  \n",
    "    recall = float(right_label_num) / all_marked_label_num\n",
    "\n",
    "    return (precision * recall) / (precision + recall)\n",
    "\n",
    "def label_index_extraction(x):\n",
    "    lst =[]\n",
    "    for i, j in enumerate(x):\n",
    "        if j == 1:\n",
    "            lst.append(i)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = embedding.shape[0]\n",
    "RNN_hidden_size = 256\n",
    "target_size = train_label.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph created\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "title = tf.placeholder(tf.int64, [None, None], \"title\")                # [batch_size x max_len]\n",
    "title_length = tf.placeholder(tf.int64, [None], \"title_length\")        # [batch_size]\n",
    "labels = tf.placeholder(tf.int64, [None, target_size], \"label\")                      # [batch_size x target_size]\n",
    "\n",
    "initializer = tf.random_uniform_initializer(-0.5, 0.5)\n",
    "embeddings = tf.get_variable(\"embeddings\", [vocab_size, 256], initializer=initializer, trainable= True)\n",
    "embeddings = embeddings.assign(char_embedding) \n",
    "\n",
    "title_embedded = tf.nn.embedding_lookup(embeddings, title)  \n",
    "\n",
    "### ------------------------------------------------------------------------------------------------------- ### \n",
    "### ---------------------------------------- sentence encoders -------------------------------------------  ###\n",
    "### ------------------------------------------------------------------------------------------------------- ### \n",
    "\n",
    "lstm_title_cell = tf.contrib.rnn.LSTMCell(RNN_hidden_size, state_is_tuple= True)\n",
    "#lstm_title_cell = tf.contrib.rnn.GRUCell(RNN_hidden_size)\n",
    "_, title_final_state = tf.nn.dynamic_rnn(lstm_title_cell, title_embedded, sequence_length = title_length, dtype=tf.float32)        \n",
    "    \n",
    "title_vector = title_final_state.h  \n",
    "title_vector = tf.layers.batch_normalization(title_vector)\n",
    "### ------------------------------------------------------------------------------------------------------- ### \n",
    "### ---------------------------------------- loss & prediction -------------------------------------------  ###\n",
    "### ------------------------------------------------------------------------------------------------------- ###     \n",
    "\n",
    "h = tf.contrib.layers.linear(title_vector, 256, activation_fn = tf.nn.relu)\n",
    "h = tf.layers.batch_normalization(h) \n",
    "logits = tf.contrib.layers.linear(h, target_size)\n",
    "\n",
    "probability = tf.nn.sigmoid(logits)\n",
    "loss = tf.losses.sigmoid_cross_entropy(logits = logits, multi_class_labels = labels)\n",
    "                                                                                    \n",
    "opt_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "saver = tf.train.Saver()\n",
    "print('graph created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Epoch 1 -----\n",
      " Train loss: 114.161326655 Time: 40.82 minute\n",
      "0.205479362458\n",
      "----- Epoch 2 -----\n",
      " Train loss: 68.2134874822 Time: 45.75 minute\n",
      "0.308159361603\n",
      "----- Epoch 3 -----\n",
      " Train loss: 62.035041003 Time: 45.67 minute\n",
      "0.320652974627\n",
      "----- Epoch 4 -----\n",
      " Train loss: 60.0603624098 Time: 45.63 minute\n",
      "0.32626442854\n",
      "----- Epoch 5 -----\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-ffb3d6647abd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minst_train_title\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle_length\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minst_train_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minst_train_labels\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mopt_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\zcakz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\zcakz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    952\u001b[0m             \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 954\u001b[1;33m             \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    955\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[1;32mC:\\Users\\zcakz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \"\"\"\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 200\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    n = train_title.shape[0]\n",
    "    val_label_index = [label_index_extraction(i) for i in list(val_label[-1000:].toarray())] \n",
    "\n",
    "    for epoch in range(15):\n",
    "        print('----- Epoch', epoch + 1, '-----')\n",
    "        total_loss = 0\n",
    "        t_0 = time.time()\n",
    "        \n",
    "        for i in range(n// (BATCH_SIZE )):\n",
    "            index_list = random.sample(range(n), BATCH_SIZE )\n",
    "            inst_train_title = [train_title[idx] for idx in index_list]\n",
    "            inst_train_length = [train_title_length[idx] for idx in index_list]\n",
    "            inst_train_labels = train_label[index_list].toarray()\n",
    "            feed_dict = {title: inst_train_title, title_length: inst_train_length, labels: inst_train_labels}\n",
    "\n",
    "            _, current_loss = sess.run([opt_op, loss], feed_dict=feed_dict)\n",
    "            total_loss = current_loss + total_loss\n",
    "            \n",
    "        print(' Train loss:', total_loss, 'Time:', round((time.time()-t_0)/60,2),'minute') \n",
    "        save_path = saver.save(sess, \"saved_models/lstm_baseline_model_%s.ckpt\" % (epoch + 1))\n",
    "        \n",
    "        inst_val_title = val_title[-1000:]\n",
    "        inst_val_length = val_title_length[-1000:]\n",
    "        val_feed_dict = {title: inst_val_title, title_length: inst_val_length}\n",
    "        val_probability = sess.run([probability], feed_dict=val_feed_dict)\n",
    "        predicted_labels = [list(np.argsort(i)[::-1][:5]) for i in val_probability[0]]\n",
    "        label_pairs = list(zip(predicted_labels, val_label_index))\n",
    "        f1 = evalation(label_pairs)\n",
    "        print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from saved_models/lstm_baseline_model_4.ckpt\n",
      "0.367834207435\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 200\n",
    "with tf.Session() as sess:  \n",
    "    saver.restore(sess, \"saved_models/lstm_baseline_model_4.ckpt\")\n",
    "    n = train_title.shape[0]\n",
    "    val_label_index = [label_index_extraction(i) for i in list(val_label[500:1000].toarray())] \n",
    "    \n",
    "    inst_val_title = val_title[500:1000]\n",
    "    inst_val_length = val_title_length[500:1000]\n",
    "    val_feed_dict = {title: inst_val_title, title_length: inst_val_length}\n",
    "    val_probability = sess.run([probability], feed_dict=val_feed_dict)\n",
    "    predicted_labels = [list(np.argsort(i)[::-1][:5]) for i in val_probability[0]]\n",
    "    label_pairs = list(zip(predicted_labels, val_label_index))\n",
    "    f1 = evalation(label_pairs)\n",
    "    print(f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
