{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "#pd.options.display.max_columns = None\n",
    "#pd.options.display.mpl_style = 'default'\n",
    "\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_ratio  = 1\n",
    "\n",
    "n = round(len(np.load('preprocessed_data/train_title_char.npy'))* training_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_title_char = np.load('preprocessed_data/train_title_char.npy')[:n]\n",
    "train_title_word = np.load('preprocessed_data/train_title_word.npy')[:n]\n",
    "test_title_char = np.load('preprocessed_data/test_title_char.npy')\n",
    "test_title_word = np.load('preprocessed_data/test_title_word.npy')\n",
    "\n",
    "train_labels = np.load('preprocessed_data/labels.npy').item()[:n]\n",
    "label_dictionary = np.load('preprocessed_data/label_dictionary.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_embedding = np.load('preprocessed_data/char_embedding.npy')\n",
    "word_embedding = np.load('preprocessed_data/word_embedding.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_len = np.load('preprocessed_data/train_length.npy')\n",
    "test_len = np.load('preprocessed_data/test_length.npy')\n",
    "\n",
    "train_title_char_len = train_len[0][:n]\n",
    "train_title_word_len = train_len[1][:n]\n",
    "test_title_char_len = test_len[0]\n",
    "test_title_word_len = test_len[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title = 'char'\n",
    "slicing_index = -20000\n",
    "train_size = train_title_char.shape[0]\n",
    "\n",
    "if title  == 'char':\n",
    "    train_title = train_title_char[:slicing_index]\n",
    "    train_title_length = train_title_char_len[:slicing_index]\n",
    "    train_label = train_labels[:slicing_index]\n",
    "    val_title = train_title_char[slicing_index:]\n",
    "    val_title_length = train_title_char_len[slicing_index:]\n",
    "    val_label = train_labels[slicing_index:]\n",
    "    test_title = test_title_char\n",
    "    test_title_length = test_title_char_len    \n",
    "    embedding = char_embedding\n",
    "elif title  == 'word':\n",
    "    train_title = train_title_word[:slicing_index]\n",
    "    train_title_length = train_title_word_len[:slicing_index]\n",
    "    train_label = train_labels[:slicing_index]\n",
    "    val_title = train_title_word[slicing_index:]\n",
    "    val_title_length = train_title_word_len[slicing_index:]\n",
    "    val_label = train_labels[slicing_index:]\n",
    "    test_title = test_title_word\n",
    "    test_title_length = test_title_word_len    \n",
    "    embedding = word_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title Charactor RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evalation(predict_label_and_marked_label_list):\n",
    "\n",
    "    right_label_num = 0  \n",
    "    right_label_at_pos_num = [0, 0, 0, 0, 0] \n",
    "    sample_num = 0   \n",
    "    all_marked_label_num = 0    \n",
    "    for predict_labels, marked_labels in predict_label_and_marked_label_list:\n",
    "        sample_num += 1\n",
    "        marked_label_set = set(marked_labels)\n",
    "        all_marked_label_num += len(marked_label_set)\n",
    "        for pos, label in zip(range(0, min(len(predict_labels), 5)), predict_labels):\n",
    "            if label in marked_label_set:     \n",
    "                right_label_num += 1\n",
    "                right_label_at_pos_num[pos] += 1\n",
    "\n",
    "    precision = 0.0\n",
    "    for pos, right_num in zip(range(0, 5), right_label_at_pos_num):\n",
    "        precision += ((right_num / float(sample_num))) / np.log(2.0 + pos)  \n",
    "    recall = float(right_label_num) / all_marked_label_num\n",
    "\n",
    "    return (precision * recall) / (precision + recall)\n",
    "\n",
    "def label_index_extraction(x):\n",
    "    lst =[]\n",
    "    for i, j in enumerate(x):\n",
    "        if j == 1:\n",
    "            lst.append(i)\n",
    "    return lst\n",
    "\n",
    "def split(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_size = embedding.shape[0]\n",
    "RNN_hidden_size = 256\n",
    "target_size = train_label.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph created\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "title = tf.placeholder(tf.int64, [None, None], \"title\")                # [batch_size x max_len]\n",
    "title_length = tf.placeholder(tf.int64, [None], \"title_length\")        # [batch_size]\n",
    "labels = tf.placeholder(tf.int64, [None, target_size], \"label\")        # [batch_size x target_size]\n",
    "\n",
    "initializer = tf.random_uniform_initializer(-0.5, 0.5)\n",
    "embeddings = tf.get_variable(\"embeddings\", [vocab_size, 256], initializer=initializer, trainable= True)\n",
    "embeddings = embeddings.assign(embedding) \n",
    "\n",
    "title_embedded = tf.nn.embedding_lookup(embeddings, title)  \n",
    "\n",
    "### ------------------------------------------------------------------------------------------------------- ### \n",
    "### ---------------------------------------- sentence encoders -------------------------------------------  ###\n",
    "### ------------------------------------------------------------------------------------------------------- ### \n",
    "\n",
    "lstm_title_cell = tf.contrib.rnn.LSTMCell(RNN_hidden_size, state_is_tuple= True)\n",
    "#lstm_title_cell = tf.contrib.rnn.GRUCell(RNN_hidden_size)\n",
    "_, title_final_state = tf.nn.dynamic_rnn(lstm_title_cell, title_embedded, sequence_length = title_length, dtype=tf.float32)        \n",
    "    \n",
    "title_vector = title_final_state.h  \n",
    "title_vector = tf.layers.batch_normalization(title_vector)\n",
    "### ------------------------------------------------------------------------------------------------------- ### \n",
    "### ---------------------------------------- loss & prediction -------------------------------------------  ###\n",
    "### ------------------------------------------------------------------------------------------------------- ###     \n",
    "\n",
    "h = tf.contrib.layers.linear(title_vector, 256, activation_fn = tf.nn.relu)\n",
    "h = tf.layers.batch_normalization(h) \n",
    "logits = tf.contrib.layers.linear(h, target_size)\n",
    "\n",
    "probability = tf.nn.sigmoid(logits)\n",
    "loss = tf.losses.sigmoid_cross_entropy(logits = logits, multi_class_labels = labels)\n",
    "                                                                                    \n",
    "opt_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "print('graph created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from saved_models/lstm_baseline_model_6.ckpt\n",
      "----- Epoch 1 -----\n",
      " Train loss: 63.3156142624 Time: 47.23 minute\n",
      "train (partial) f1:  0.435999375943\n",
      "validaton f1:  0.36964792703\n",
      "----- Epoch 2 -----\n",
      " Train loss: 62.9856615735 Time: 50.08 minute\n",
      "train (partial) f1:  0.438510953905\n",
      "validaton f1:  0.36964792703\n",
      "----- Epoch 3 -----\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-266ec723db14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minst_train_title\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle_length\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minst_train_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minst_train_labels\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mopt_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\zcakz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\zcakz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\zcakz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\zcakz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\zcakz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 200\n",
    "with tf.Session() as sess:  \n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, \"saved_models/lstm_baseline_model_6.ckpt\")\n",
    "    n = train_title.shape[0]\n",
    "    \n",
    "    part_train_title = train_title[:1000]\n",
    "    part_train_length = train_title_length[:1000]\n",
    "    train_feed_dict = {title: part_train_title, title_length: part_train_length}\n",
    "    \n",
    "    train_label_index = [label_index_extraction(i) for i in list(train_label[:1000].toarray())] \n",
    "    val_label_index = [label_index_extraction(i) for i in list(val_label.toarray())] \n",
    "    val_title_split = list(split(val_title, 20))\n",
    "    val_title_length_split = list(split(val_title_length, 20))\n",
    "    val_predicted = []\n",
    "    \n",
    "    for epoch in range(15):\n",
    "        print('----- Epoch', epoch + 1, '-----')\n",
    "        total_loss = 0\n",
    "        t_0 = time.time()\n",
    "        \n",
    "        for i in range(n// (BATCH_SIZE )):\n",
    "            index_list = random.sample(range(n), BATCH_SIZE )\n",
    "            inst_train_title = [train_title[idx] for idx in index_list]\n",
    "            inst_train_length = [train_title_length[idx] for idx in index_list]\n",
    "            inst_train_labels = train_label[index_list].toarray()\n",
    "            feed_dict = {title: inst_train_title, title_length: inst_train_length, labels: inst_train_labels}\n",
    "\n",
    "            _, current_loss = sess.run([opt_op, loss], feed_dict=feed_dict)\n",
    "            total_loss = current_loss + total_loss\n",
    "            \n",
    "        print(' Train loss:', total_loss, 'Time:', round((time.time()-t_0)/60,2),'minute') \n",
    "        #save_path = saver.save(sess, \"saved_models/lstm_baseline_model_%s.ckpt\" % (epoch + 1))\n",
    "              \n",
    "        train_probability = sess.run([probability], feed_dict=train_feed_dict)\n",
    "        train_predicted_labels = [list(np.argsort(i)[::-1][:5]) for i in train_probability[0]]\n",
    "        train_label_pairs = list(zip(train_predicted_labels, train_label_index))\n",
    "        train_f1 = evalation(train_label_pairs)\n",
    "        print('train (partial) f1: ', train_f1)\n",
    "        \n",
    "        for i in range(20):\n",
    "            val_feed_dict = {title: val_title_split[i], title_length: val_title_length_split[i]}\n",
    "            val_probability = sess.run([probability], feed_dict=val_feed_dict)\n",
    "            val_predicted_labels = [list(np.argsort(i)[::-1][:5]) for i in val_probability[0]]\n",
    "            val_predicted = val_predicted + val_predicted_labels \n",
    "        val_label_pairs = list(zip(val_predicted, val_label_index))\n",
    "        val_f1 = evalation(val_label_pairs)\n",
    "        print('validaton f1: ', val_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval Prediction"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "BATCH_SIZE = 200\n",
    "with tf.Session() as sess:  \n",
    "    saver.restore(sess, \"saved_models/lstm_baseline_model_6.ckpt\")\n",
    "    test_predicted = []\n",
    "    test_title_split = list(split(test_title, 200))\n",
    "    test_title_length_split = list(split(test_title_length, 200))\n",
    "    \n",
    "    for i in range(200):\n",
    "        test_feed_dict = {title: test_title_split[i], title_length: test_title_length_split[i]}\n",
    "        test_current_probability = sess.run([probability], feed_dict=test_feed_dict)\n",
    "        test_predicted_labels = [list(np.argsort(i)[::-1][:5]) for i in test_current_probability[0]]\n",
    "        test_predicted = test_predicted + test_predicted_labels       \n",
    "\n",
    "df_eval = pd.read_csv('data\\question_eval_set.txt', sep=\"\\t\", header = None, names = ['question_id', 'title_char_id','title_word_id','desc_char_id','desc_word_id'])\n",
    "output = pd.concat([df_eval['question_id'], pd.DataFrame(test_predicted)], axis=1)\n",
    "\n",
    "for i in range(5):\n",
    "    output[i] = output[i].map(lambda x: label_dictionary[x])\n",
    "    \n",
    "output.to_csv('predicted.csv', header = False, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
